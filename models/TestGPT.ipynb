{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7523b424-db9e-4ca8-873f-61f8aa428261",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Model \u001b[38;5;28;01mas\u001b[39;00m OpenAIGPT2Model\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Config\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_gpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTPreTrainedModel\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSonnetLayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SonnetLayer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "#colab 환경에서 config.py를 import한다.\n",
    "#sys.path.append('/content/drive/MyDrive/nlp2025-1-main')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Model as OpenAIGPT2Model\n",
    "\n",
    "from config import GPT2Config\n",
    "from models.base_gpt import GPTPreTrainedModel\n",
    "from modules.SonnetLayer import SonnetLayer\n",
    "from utils import get_extended_attention_mask\n",
    "\n",
    "\n",
    "class GPT2Model(GPTPreTrainedModel):\n",
    "  \"\"\"\n",
    "  GPT 모델은 문장 내 각 토큰에 대한 최종 임베딩을 반환한다.\n",
    "\n",
    "  모델 구성은 다음과 같다:\n",
    "  1. 임베딩 층 (self.embed 에서 사용).\n",
    "  2. n 개의 GPT 층의 적층 (self.encode 에서 사용).\n",
    "  3. [CLS] 토큰에 대한 선형변환 층(self.forward 에서 그대로 사용).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.config = config\n",
    "\n",
    "    # Embedding layers.\n",
    "    self.word_embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "    self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "    self.embed_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # (1, position_임베딩_길이)의 position_ids는 학습되지 않는 상수이므로 버퍼에 저장해둔다.\n",
    "    position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
    "    self.register_buffer('position_ids', position_ids)\n",
    "\n",
    "    # GPT-2 layers.\n",
    "    self.gpt_layers = nn.ModuleList([SonnetLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    # [CLS] 토큰 변환.\n",
    "    self.pooler_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.pooler_af = nn.Tanh()\n",
    "\n",
    "    # Final layer norm.\n",
    "    self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def embed(self, input_ids):\n",
    "    input_shape = input_ids.size()\n",
    "    seq_length = input_shape[1]\n",
    "\n",
    "    # 단어 임베딩 가져오기\n",
    "    inputs_embeds = self.word_embedding(input_ids)\n",
    "\n",
    "    # 위치 ID 가져오기 (seq_length만큼)\n",
    "    pos_ids = self.position_ids[:, :seq_length]\n",
    "    # 위치 임베딩 가져오기\n",
    "    pos_embeds = self.pos_embedding(pos_ids)\n",
    "\n",
    "    # 단어 임베딩과 위치 임베딩 더하기\n",
    "    final_embeds = inputs_embeds + pos_embeds\n",
    "\n",
    "    # 드롭아웃 적용 (수동 구현)\n",
    "    if self.training:\n",
    "        # 훈련 모드일 때만 드롭아웃 적용\n",
    "        dropout_prob = self.embed_dropout.p\n",
    "        # 베르누이 분포로 마스크 생성 (1-p 확률로 1, p 확률로 0)\n",
    "        dropout_mask = torch.bernoulli(torch.full_like(final_embeds, 1.0 - dropout_prob))\n",
    "        # 살아남은 뉴런들을 1/(1-p)로 스케일링\n",
    "        final_embeds = final_embeds * dropout_mask / (1.0 - dropout_prob)\n",
    "\n",
    "    return final_embeds\n",
    "\n",
    "\n",
    "  def encode(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: 임베딩 층으로부터의 출력 [batch_size, seq_len, hidden_size]\n",
    "    attention_mask: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # self-attention을 위한 extended attention mask를 구한다.\n",
    "    # 크기 [batch_size, 1, 1, seq_len]인 extended_attention_mask를 반환.\n",
    "    # (0 값이 포함된) non-padding token과 (큰 음수들로 된) padding token을 구별할 것.\n",
    "    extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask, self.dtype)\n",
    "\n",
    "    # encoder 층을 통해 hidden states 전달.\n",
    "    for i, layer_module in enumerate(self.gpt_layers):\n",
    "      # 마지막 bert_layer에서 인코딩를 가져다가 다음 층에 주입.\n",
    "      hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    input_ids: [batch_size, seq_len], seq_len은 batch의 최대 길이\n",
    "    attention_mask: input_ids 와 크기가 같으며, 1 은 non-padding token을, 0 은 padding token을 나타낸다.\n",
    "    \"\"\"\n",
    "    # 각 입렵 토큰에 대한 임베딩 구하기기\n",
    "    embedding_output = self.embed(input_ids=input_ids)\n",
    "\n",
    "    # GPYLayers의 stack인 트랜스포머에 주입.\n",
    "    sequence_output = self.encode(embedding_output, attention_mask=attention_mask)\n",
    "    sequence_output = self.final_layer_norm(sequence_output)\n",
    "\n",
    "    # 마지막 토큰의 hidden state 구하기.\n",
    "    last_non_pad_idx = attention_mask.sum(dim=1) - 1  # 마지막 인덱스를 구하려면 1을 뺀다.\n",
    "    last_token = sequence_output[torch.arange(sequence_output.shape[0]), last_non_pad_idx]\n",
    "\n",
    "    return {'last_hidden_state': sequence_output, 'last_token': last_token}\n",
    "\n",
    "  def hidden_state_to_token(self, hidden_state):\n",
    "    \"\"\"\n",
    "    GPT-2 uses weight tying with the input word embeddings. The logits are the dot product between output hidden states\n",
    "    and the word embedding weights:\n",
    "    GPT-2는 입력 단어 임베딩과 가중치 공유(weight tying)를 사용한다.\n",
    "    로짓(logits)은 출력 은닉 상태와 단어 임베딩 가중치 간의 내적(dot product).\n",
    "\n",
    "      return hidden_state(s) * E^T\n",
    "    \"\"\"\n",
    "    # 단어 임베딩 가중치 (E) 가져오기\n",
    "    embed_weights = self.word_embedding.weight\n",
    "\n",
    "    # 로짓 계산: hidden_state * (E^T)\n",
    "    # embed_weights.shape: (vocab_size, hidden_size)\n",
    "    # 전치하면: (hidden_size, vocab_size)\n",
    "    # 수동으로 전치 구현 (transpose 대신)\n",
    "    embed_weights_transposed = embed_weights.permute(1, 0)  # (hidden_size, vocab_size)\n",
    "\n",
    "    # 행렬곱 계산: hidden_state\n",
    "    logits = torch.matmul(hidden_state, embed_weights_transposed)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model='gpt2', d=768, l=12, num_heads=12):\n",
    "    gpt_model = OpenAIGPT2Model.from_pretrained(model).eval()\n",
    "    our_model = GPT2Model(GPT2Config(hidden_size=d, num_hidden_layers=l,num_attention_heads=num_heads,\n",
    "                                     intermediate_size=d*3)).eval()\n",
    "\n",
    "    # Load word and positional embeddings.\n",
    "    our_model.word_embedding.load_state_dict(gpt_model.wte.state_dict())\n",
    "    our_model.pos_embedding.load_state_dict(gpt_model.wpe.state_dict())\n",
    "\n",
    "    for i in range(l):\n",
    "      l = our_model.gpt_layers[i]\n",
    "      # Q, K, V 가중치를 conv1d에서 3개의 선형 프로젝션으로 재매핑.\n",
    "      l.self_attention.query.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, :d].T\n",
    "      l.self_attention.query.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][:d]\n",
    "      l.self_attention.key.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d:d*2].T\n",
    "      l.self_attention.key.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d:d*2]\n",
    "      l.self_attention.value.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d*2:].T\n",
    "      l.self_attention.value.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d*2:]\n",
    "\n",
    "      # MHA의 마지막 dense layer를 재매핑.\n",
    "      l.attention_dense.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.weight'].T\n",
    "      l.attention_dense.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.bias']\n",
    "\n",
    "      # Attention layer norm을 재매핑.\n",
    "      l.attention_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_1.weight']\n",
    "      l.attention_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_1.bias']\n",
    "\n",
    "      # Post-attention MLP layer들을 재매핑\n",
    "      l.interm_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.weight'].T\n",
    "      l.interm_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.bias']\n",
    "      l.out_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.weight'].T\n",
    "      l.out_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.bias']\n",
    "\n",
    "      # 두번째 layer norm weights를 재매핑.\n",
    "      l.out_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_2.weight']\n",
    "      l.out_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_2.bias']\n",
    "\n",
    "    # 마지막 layer norm 값들을 재매핑.\n",
    "    our_model.final_layer_norm.weight.data = gpt_model.state_dict()['ln_f.weight']\n",
    "    our_model.final_layer_norm.bias.data = gpt_model.state_dict()['ln_f.bias']\n",
    "\n",
    "    return our_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5bf414-a628-4870-81ae-e8606e57892c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_final)",
   "language": "python",
   "name": "nlp_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
