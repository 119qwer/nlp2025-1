{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244d1ec-f3ae-441b-bfae-5d93e47b73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Paraphrase detection을 위한 시작 코드.\n",
    "\n",
    "고려 사항:\n",
    " - ParaphraseGPT: 여러분이 구현한 GPT-2 분류 모델 .\n",
    " - train: Quora paraphrase detection 데이터셋에서 ParaphraseGPT를 훈련시키는 절차.\n",
    " - test: Test 절차. 프로젝트 결과 제출에 필요한 파일들을 생성함.\n",
    "\n",
    "실행:\n",
    "  `python paraphrase_detection.py --use_gpu`\n",
    "ParaphraseGPT model을 훈련 및 평가하고, 필요한 제출용 파일을 작성한다.\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import (\n",
    "  ParaphraseDetectionDataset,\n",
    "  ParaphraseDetectionTestDataset,\n",
    "  load_paraphrase_data\n",
    ")\n",
    "from evaluation import model_eval_paraphrase, model_test_paraphrase\n",
    "from models.gpt2 import GPT2Model\n",
    "\n",
    "from optimizer import AdamW\n",
    "\n",
    "TQDM_DISABLE = False\n",
    "\n",
    "# Fix the random seed.\n",
    "def seed_everything(seed=11711):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class ParaphraseGPT(nn.Module):\n",
    "  \"\"\"Paraphrase Detection을 위해 설계된 여러분의 GPT-2 Model.\"\"\"\n",
    "\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    self.gpt = GPT2Model.from_pretrained(model=args.model_size, d=args.d, l=args.l, num_heads=args.num_heads)\n",
    "    self.paraphrase_detection_head = nn.Linear(args.d, 2)  # Paraphrase detection 의 출력은 두 가지: 1 (yes) or 0 (no).\n",
    "\n",
    "    # 기본적으로, 전체 모델을 finetuning 한다.\n",
    "    for param in self.gpt.parameters():\n",
    "      param.requires_grad = True\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Paraphrase detection을 위한 forward 함수.\n",
    "    \n",
    "    입력:\n",
    "        input_ids: 토큰화된 입력 문장들의 ID\n",
    "        attention_mask: attention mask\n",
    "    \n",
    "    출력:\n",
    "        logits: 레이블 인덱스에 맞는 logits (3919: \"no\", 8505: \"yes\")\n",
    "    \"\"\"\n",
    "    # GPT 모델을 통해 hidden states 얻기\n",
    "    gpt_outputs = self.gpt(input_ids, attention_mask)\n",
    "    \n",
    "    # hidden states 추출\n",
    "    if isinstance(gpt_outputs, dict):\n",
    "        hidden_states = gpt_outputs['last_hidden_state']\n",
    "    else:\n",
    "        hidden_states = gpt_outputs[0]\n",
    "    \n",
    "    # 마지막 토큰의 hidden state만 사용\n",
    "    last_token_hidden = hidden_states[:, -1, :]\n",
    "    \n",
    "    # classification head를 통과시켜 2-class logits 생성\n",
    "    binary_logits = self.paraphrase_detection_head(last_token_hidden)\n",
    "    \n",
    "    # 레이블 인덱스(3919, 8505)에 맞는 logits 텐서 생성\n",
    "    batch_size = input_ids.size(0)\n",
    "    device = input_ids.device\n",
    "    \n",
    "    # 전체 vocab size만큼의 logits 텐서 생성 (매우 낮은 값으로 초기화)\n",
    "    vocab_size = 50257  # GPT-2 vocab size\n",
    "    full_logits = torch.full((batch_size, vocab_size), -1e9, device=device)\n",
    "    \n",
    "    # 레이블 위치에 실제 logits 값 할당\n",
    "    # binary_logits[:, 0] -> 3919 (no)\n",
    "    # binary_logits[:, 1] -> 8505 (yes)\n",
    "    full_logits[:, 3919] = binary_logits[:, 0]\n",
    "    full_logits[:, 8505] = binary_logits[:, 1]\n",
    "    \n",
    "    return full_logits\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, args, filepath):\n",
    "  save_info = {\n",
    "    'model': model.state_dict(),\n",
    "    'optim': optimizer.state_dict(),\n",
    "    'args': args,\n",
    "    'system_rng': random.getstate(),\n",
    "    'numpy_rng': np.random.get_state(),\n",
    "    'torch_rng': torch.random.get_rng_state(),\n",
    "  }\n",
    "\n",
    "  torch.save(save_info, filepath)\n",
    "  print(f\"save the model to {filepath}\")\n",
    "\n",
    "\n",
    "def train(args):\n",
    "  \"\"\"Quora 데이터셋에서 Paraphrase Detection을 위한 GPT-2 훈련.\"\"\"\n",
    "  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "  # 데이터, 해당 데이터셋 및 데이터로드 생성하기.\n",
    "  para_train_data = load_paraphrase_data(args.para_train)\n",
    "  para_dev_data = load_paraphrase_data(args.para_dev)\n",
    "\n",
    "  para_train_data = ParaphraseDetectionDataset(para_train_data, args)\n",
    "  para_dev_data = ParaphraseDetectionDataset(para_dev_data, args)\n",
    "\n",
    "  para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                     collate_fn=para_train_data.collate_fn)\n",
    "  para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                   collate_fn=para_dev_data.collate_fn)\n",
    "\n",
    "  args = add_arguments(args)\n",
    "  model = ParaphraseGPT(args)\n",
    "  model = model.to(device)\n",
    "\n",
    "  lr = args.lr\n",
    "  optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.)\n",
    "  best_dev_acc = 0\n",
    "\n",
    "  for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in tqdm(para_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "      # 입력을 가져와서 GPU로 보내기(이 모델을 CPU에서 훈련시키는 것을 권장하지 않는다).\n",
    "      b_ids, b_mask, labels = batch['token_ids'], batch['attention_mask'], batch['labels'].flatten()\n",
    "      b_ids = b_ids.to(device)\n",
    "      b_mask = b_mask.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # 손실, 그래디언트를 계산하고 모델 파라미터 업데이트. \n",
    "      optimizer.zero_grad()\n",
    "      logits = model(b_ids, b_mask)\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "      loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "      num_batches += 1\n",
    "\n",
    "    train_loss = train_loss / num_batches\n",
    "\n",
    "    dev_acc, dev_f1, *_ = model_eval_paraphrase(para_dev_dataloader, model, device)\n",
    "\n",
    "    if dev_acc > best_dev_acc:\n",
    "      best_dev_acc = dev_acc\n",
    "      save_model(model, optimizer, args, args.filepath)\n",
    "\n",
    "    print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, dev acc :: {dev_acc :.3f}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(args):\n",
    "  \"\"\"Evaluate your model on the dev and test datasets; save the predictions to disk.\"\"\"\n",
    "  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "  saved = torch.load(args.filepath)\n",
    "\n",
    "  model = ParaphraseGPT(saved['args'])\n",
    "  model.load_state_dict(saved['model'])\n",
    "  model = model.to(device)\n",
    "  model.eval()\n",
    "  print(f\"Loaded model to test from {args.filepath}\")\n",
    "\n",
    "  para_dev_data = load_paraphrase_data(args.para_dev)\n",
    "  para_test_data = load_paraphrase_data(args.para_test, split='test')\n",
    "\n",
    "  para_dev_data = ParaphraseDetectionDataset(para_dev_data, args)\n",
    "  para_test_data = ParaphraseDetectionTestDataset(para_test_data, args)\n",
    "\n",
    "  para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                   collate_fn=para_dev_data.collate_fn)\n",
    "  para_test_dataloader = DataLoader(para_test_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                    collate_fn=para_test_data.collate_fn)\n",
    "\n",
    "  dev_para_acc, _, dev_para_y_pred, _, dev_para_sent_ids = model_eval_paraphrase(para_dev_dataloader, model, device)\n",
    "  print(f\"dev paraphrase acc :: {dev_para_acc :.3f}\")\n",
    "  test_para_y_pred, test_para_sent_ids = model_test_paraphrase(para_test_dataloader, model, device)\n",
    "\n",
    "  with open(args.para_dev_out, \"w+\") as f:\n",
    "    f.write(f\"id \\t Predicted_Is_Paraphrase \\n\")\n",
    "    for p, s in zip(dev_para_sent_ids, dev_para_y_pred):\n",
    "      f.write(f\"{p}, {s} \\n\")\n",
    "\n",
    "  with open(args.para_test_out, \"w+\") as f:\n",
    "    f.write(f\"id \\t Predicted_Is_Paraphrase \\n\")\n",
    "    for p, s in zip(test_para_sent_ids, test_para_y_pred):\n",
    "      f.write(f\"{p}, {s} \\n\")\n",
    "\n",
    "\n",
    "def get_args():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\"--para_train\", type=str, default=\"data/quora-train.csv\")\n",
    "  parser.add_argument(\"--para_dev\", type=str, default=\"data/quora-dev.csv\")\n",
    "  parser.add_argument(\"--para_test\", type=str, default=\"data/quora-test-student.csv\")\n",
    "  parser.add_argument(\"--para_dev_out\", type=str, default=\"predictions/para-dev-output.csv\")\n",
    "  parser.add_argument(\"--para_test_out\", type=str, default=\"predictions/para-test-output.csv\")\n",
    "\n",
    "  parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "  parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "  parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "  parser.add_argument(\"--batch_size\", help='sst: 64, cfimdb: 8 can fit a 12GB GPU', type=int, default=8)\n",
    "  parser.add_argument(\"--lr\", type=float, help=\"learning rate\", default=1e-5)\n",
    "  parser.add_argument(\"--model_size\", type=str,\n",
    "                      help=\"The model size as specified on hugging face. DO NOT use the xl model.\",\n",
    "                      choices=['gpt2', 'gpt2-medium', 'gpt2-large'], default='gpt2')\n",
    "\n",
    "  # Jupyter Notebook에서 실행할 때는 빈 리스트로 parse\n",
    "  import sys\n",
    "  if 'ipykernel' in sys.modules:\n",
    "    args = parser.parse_args([])\n",
    "  else:\n",
    "    args = parser.parse_args()\n",
    "  \n",
    "  return args\n",
    "\n",
    "\n",
    "def add_arguments(args):\n",
    "  \"\"\"모델 크기에 따라 결정되는 인수들을 추가.\"\"\"\n",
    "  if args.model_size == 'gpt2':\n",
    "    args.d = 768\n",
    "    args.l = 12\n",
    "    args.num_heads = 12\n",
    "  elif args.model_size == 'gpt2-medium':\n",
    "    args.d = 1024\n",
    "    args.l = 24\n",
    "    args.num_heads = 16\n",
    "  elif args.model_size == 'gpt2-large':\n",
    "    args.d = 1280\n",
    "    args.l = 36\n",
    "    args.num_heads = 20\n",
    "  else:\n",
    "    raise Exception(f'{args.model_size} is not supported.')\n",
    "  return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  args = get_args()\n",
    "  args.filepath = f'{args.epochs}-{args.lr}-paraphrase.pt'  # 경로명 저장.\n",
    "  seed_everything(args.seed)  # 재현성을 위한 random seed 고정.\n",
    "  train(args)\n",
    "  test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_final]",
   "language": "python",
   "name": "conda-env-nlp_final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
