{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431a5c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n소넷 생성을 위한 시작 코드.\\n\\n실행:\\n  `python sonnet_generation.py --use_gpu`\\n\\ntrains your SonnetGPT model and writes the required submission files.\\nSonnetGPT 모델을 훈련하고, 필요한 제출용 파일을 작성한다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "소넷 생성을 위한 시작 코드.\n",
    "\n",
    "실행:\n",
    "  `python sonnet_generation.py --use_gpu`\n",
    "\n",
    "trains your SonnetGPT model and writes the required submission files.\n",
    "SonnetGPT 모델을 훈련하고, 필요한 제출용 파일을 작성한다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca3442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d40219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02af52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d773fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "  SonnetsDataset,\n",
    ")\n",
    "from models.gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f60990",
   "metadata": {},
   "outputs": [],
   "source": [
    "TQDM_DISABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772b1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현성을 위한 random seed 고정.\n",
    "def seed_everything(seed=11711):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964eec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonnetGPT(nn.Module):\n",
    "  \"\"\"Sonnet 생성을 위해 설계된 여러분의 GPT-2 모델.\"\"\"\n",
    "\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    self.gpt = GPT2Model.from_pretrained(model=args.model_size, d=args.d, l=args.l, num_heads=args.num_heads)\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # 기본적으로, 전체 모델을 fine-tuning한다. TODO: 이것은 좋은 생각이 아닌 것 같다.\n",
    "    for param in self.gpt.parameters():\n",
    "      param.requires_grad = False #여기 true인데 임시로 False로 바꾼 거임@@@@@@\n",
    "        \n",
    "    # 첫번째 방법 : requires_grad를 False로 하고  마지막 출력 레이어만 학습하기(last layer tuning)\n",
    "    # 두번째 방법 : Transformer 블록 사이에 adapter를 삽입하고 이부분만 학습(Adapter tuning)\n",
    "    # 세번째 방법 : 가중치 행렬을 rank 분해해서 일부분만 학습(LoRA)\n",
    "    # 이를 위해서는 일부분만 학습하는 방법을 알아야 함\n",
    "    # 첫 방법은 데이터가 적을 때 좋고, 두번째 방법은 다양한 스타일에 좋고, 3번째는 성능과 속도가 좋다.\n",
    "    # 소네트 스타일만 사용할 것이기 때문에 현재는 첫번째와 두번째만 고려 사항이다.\n",
    "\n",
    "    # 첫번째 방법\n",
    "    # # 마지막 레이어만 fine-tuning 설정 (gpt2를 보면 gpt_layers 변수에 레이어를 저장하고 있고 거기서 마지막 레이어만 꺼내서 파인튜닝하는 것)\n",
    "    # for param in self.gpt.gpt_layers[-1].parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "    # 3번째 방법 (LoRa가 좋은 이유는 사전학습된 GPT-2는 너무 큰데 소네트 데이터는 그에 비해 작아서 학습이 안좋은데 저랭크 가중치만 학습하여 효율이 좋음)\n",
    "    # 기존 가중치를 동결하고 작은 랭크 가중치만 학습할 수 있게 하는 LoRALinear클래스를 만들고 우리가 만든 self_attention클래스의 \n",
    "    # query와 value에 self.query = LoRALinear(config.hidden_size, self.all_head_size, r=8, alpha=32) 와 같이 적용기기\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "      ### 완성시켜야 할 빈 코드 블록\n",
    "    \"\"\"\n",
    "    ParaphraseGPT의 forward pass와 유사하지만, 여기서는 시퀀스의 마지막 토큰뿐만 아니라 시퀀스의 각 토큰에 대한 logit을 생성하려고 한다.\n",
    "    이를 통해, 마지막 토큰에 대한 다음 토큰의 분포만 학습하는 것이 아니라, 모델은 소네트를 구성하는 자연어 분포를 학습할 수 있다.\n",
    "    \"\"\"\n",
    "    # 1. 임베딩\n",
    "    embedding_output = self.gpt.embed(input_ids=input_ids)\n",
    "\n",
    "    # 2. 트랜스포머 인코딩\n",
    "    hidden_states = self.gpt.encode(embedding_output, attention_mask=attention_mask)\n",
    "    hidden_states = self.gpt.final_layer_norm(hidden_states)\n",
    "\n",
    "    # 3. 로짓 계산 (vocab size로 투사)\n",
    "    logits = self.gpt.hidden_state_to_token(hidden_states)\n",
    "    return logits;\n",
    "\n",
    "\n",
    "  def get_device(self):\n",
    "    for param in self.gpt.parameters():\n",
    "      return param.device\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self, encoding, temperature=0.7, top_p=0.9, max_length=128):\n",
    "    \"\"\"\n",
    "    top-p sampling 과 softmax temperature를 사용하여 새로운 소넷을 생성한다.\n",
    "\n",
    "    TODO: 지금 이 방법은 기대 이하일 수 있다. 영감을 얻기 위해 Hugging Face의 model.generate(...) 함수를 참고해도 좋겠다.\n",
    "        여러 시퀀스를 생성하고 beam search를 통해 최적의 시퀀스를 선택하는 것도 좋은 한 가지 방법이다.\n",
    "        Top-k 샘플링 역시 또 다른 방법이며, 그 외에도 많은 접근법이 있다.\n",
    "    \"\"\"\n",
    "    token_ids = encoding.to(self.get_device())\n",
    "    attention_mask = torch.ones(token_ids.shape, dtype=torch.int64).to(self.get_device())\n",
    "\n",
    "\n",
    "    for _ in range(max_length):\n",
    "      # logits을 구하기 위한 forward pass.\n",
    "      logits_sequence = self.forward(token_ids, attention_mask)\n",
    "      logits_last_token = logits_sequence[:, -1, :] / temperature  # Apply temperature scaling\n",
    "\n",
    "      # Convert logits to probabilities\n",
    "      probs = torch.nn.functional.softmax(logits_last_token, dim=-1)\n",
    "\n",
    "      # Top-p (nucleus) sampling\n",
    "      sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "      cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "      top_p_mask = cumulative_probs <= top_p\n",
    "      top_p_mask[..., 1:] = top_p_mask[..., :-1].clone()  # Shift mask right for proper thresholding\n",
    "      top_p_mask[..., 0] = True  # Always include the highest probability token\n",
    "      filtered_probs = sorted_probs * top_p_mask  # Zero out unlikely tokens\n",
    "      filtered_probs /= filtered_probs.sum(dim=-1, keepdim=True)  # Normalize probabilities\n",
    "\n",
    "      # Sample from filtered distribution\n",
    "      sampled_index = torch.multinomial(filtered_probs, 1)\n",
    "      sampled_token = sorted_indices.gather(dim=-1, index=sampled_index)\n",
    "\n",
    "      # Stop if end-of-sequence token is reached\n",
    "      if sampled_token.item() == self.tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "      # Append sampled token\n",
    "      token_ids = torch.cat([token_ids, sampled_token], dim=1)\n",
    "      attention_mask = torch.cat(\n",
    "        [attention_mask, torch.ones((1, 1), dtype=torch.int64).to(self.get_device())], dim=1\n",
    "      )\n",
    "\n",
    "    generated_output = self.tokenizer.decode(token_ids[0].cpu().numpy().tolist())[3:]\n",
    "    return token_ids, generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03723405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, args, filepath):\n",
    "  save_info = {\n",
    "    'model': model.state_dict(),\n",
    "    'optim': optimizer.state_dict(),\n",
    "    'args': args,\n",
    "    'system_rng': random.getstate(),\n",
    "    'numpy_rng': np.random.get_state(),\n",
    "    'torch_rng': torch.random.get_rng_state(),\n",
    "  }\n",
    "\n",
    "  torch.save(save_info, filepath)\n",
    "  print(f\"save the model to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe76a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "  \"\"\"Sonnet 데이터셋에서 소넷 생성을 위해 GPT-2 훈련.\"\"\" \n",
    "  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "  # 데이터, 해당 데이터셋 및 데이터로드 생성하기.\n",
    "  sonnet_dataset = SonnetsDataset(args.sonnet_path)\n",
    "  sonnet_dataloader = DataLoader(sonnet_dataset, shuffle=True, batch_size=args.batch_size,\n",
    "                                 collate_fn=sonnet_dataset.collate_fn)\n",
    "\n",
    "  # held-out 데이터셋 만들기: 처음 3 줄만 있다. 나머지를 채우는 것은 여러분 몫이다!\n",
    "  held_out_sonnet_dataset = SonnetsDataset(args.held_out_sonnet_path)\n",
    "\n",
    "  args = add_arguments(args)\n",
    "  model = SonnetGPT(args)\n",
    "  model = model.to(device)\n",
    "    \n",
    "  #   #테스트 용 드드\n",
    "  # for name, param in model.named_parameters():\n",
    "  #   if 'lora' in name:\n",
    "  #       print(f\"{name}, requires_grad={param.requires_grad}, shape={param.shape}\")\n",
    "\n",
    "  lr = args.lr\n",
    "#####수정 부분(아래 한줄 활성화하고 두줄은 지우면 원래대로 됨)\n",
    "  optimizer = AdamW(model.parameters(), lr=lr)\n",
    "  # # 'lora_'라는 이름이 붙은 파라미터만 학습 대상\n",
    "  # lora_params = [p for n, p in model.named_parameters() if 'lora_' in n and p.requires_grad]\n",
    "  # optimizer = AdamW(lora_params, lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "  for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(sonnet_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "      # 입력을 가져와서 GPU로 보내기(이 모델을 CPU에서 훈련시키는 것을 권장하지 않는다).\n",
    "      b_ids, b_mask = batch['token_ids'], batch['attention_mask']\n",
    "      b_ids = b_ids.to(device)\n",
    "      b_mask = b_mask.to(device)\n",
    "\n",
    "      # 손실, 그래디언트를 계산하고 모델 파라미터 업데이트.\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(b_ids, b_mask)\n",
    "      logits = rearrange(logits[:, :-1].contiguous(), 'b t d -> (b t) d')  # 시퀀스의 마지막 예측은 무시한다.\n",
    "      labels = b_ids[:, 1:].contiguous().flatten()  # 레이블을 구성하기 위해 첫번째 토큰을 무시한다.\n",
    "      loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "      num_batches += 1\n",
    "\n",
    "    train_loss = train_loss / num_batches\n",
    "    print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}.\")\n",
    "    print('Generating several output sonnets...')\n",
    "    model.eval()\n",
    "    for batch in held_out_sonnet_dataset:\n",
    "      encoding = model.tokenizer(batch[1], return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "      output = model.generate(encoding['input_ids'], temperature=args.temperature, top_p=args.top_p)\n",
    "      print(f'{batch[1]}{output[1]}\\n\\n')\n",
    "\n",
    "    # TODO: 소넷의 작은 테이터셋에서 과적합을 방지하기 위한 종료 조건을 생각하시오.\n",
    "    save_model(model, optimizer, args, f'{epoch}_{args.filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "386db23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_submission_sonnets(args):\n",
    "  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "  saved = torch.load(f'{args.epochs-1}_{args.filepath}', weights_only=False)\n",
    "\n",
    "  model = SonnetGPT(saved['args'])\n",
    "  model.load_state_dict(saved['model'])\n",
    "  model = model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  # held-out 데이터셋 만들기: 처음 3 줄만 있다. 나머지를 채우는 것은 여러분 몫이다!\n",
    "  held_out_sonnet_dataset = SonnetsDataset(args.held_out_sonnet_path)\n",
    "\n",
    "  generated_sonnets = []\n",
    "  for batch in held_out_sonnet_dataset:\n",
    "    sonnet_id = batch[0]\n",
    "    encoding = model.tokenizer(batch[1], return_tensors='pt', padding=False, truncation=True).to(device)\n",
    "    output = model.generate(encoding['input_ids'], temperature=args.temperature, top_p=args.top_p)[0][0]\n",
    "    decoded_output = model.tokenizer.decode(output)\n",
    "    full_sonnet = f'{decoded_output}\\n\\n'\n",
    "    generated_sonnets.append((sonnet_id, full_sonnet))\n",
    "\n",
    "    print(f'{decoded_output}\\n\\n')\n",
    "\n",
    "  with open(args.sonnet_out, \"w+\") as f:\n",
    "    f.write(f\"--Generated Sonnets-- \\n\\n\")\n",
    "    for sonnet in generated_sonnets:\n",
    "      f.write(f\"\\n{sonnet[0]}\\n\")\n",
    "      f.write(sonnet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5445bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\"--sonnet_path\", type=str, default=\"data/sonnets.txt\")\n",
    "  parser.add_argument(\"--held_out_sonnet_path\", type=str, default=\"data/sonnets_held_out.txt\")\n",
    "  parser.add_argument(\"--sonnet_out\", type=str, default=\"predictions/generated_sonnets.txt\")\n",
    "\n",
    "  parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "  parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "  parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "  # Generation parameters.\n",
    "  parser.add_argument(\"--temperature\", type=float, help=\"softmax temperature.\", default=1.2)\n",
    "  parser.add_argument(\"--top_p\", type=float, help=\"Cumulative probability distribution for nucleus sampling.\",\n",
    "                      default=0.9)\n",
    "\n",
    "  parser.add_argument(\"--batch_size\", help='The training batch size.', type=int, default=8)\n",
    "  parser.add_argument(\"--lr\", type=float, help=\"learning rate\", default=1e-5)\n",
    "  parser.add_argument(\"--model_size\", type=str, help=\"The model size as specified on hugging face.\",\n",
    "                      choices=['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'], default='gpt2')\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88346c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arguments(args):\n",
    "  \"\"\"Add arguments that are deterministic on model size.\"\"\"\n",
    "  if args.model_size == 'gpt2':\n",
    "    args.d = 768\n",
    "    args.l = 12\n",
    "    args.num_heads = 12\n",
    "  elif args.model_size == 'gpt2-medium':\n",
    "    args.d = 1024\n",
    "    args.l = 24\n",
    "    args.num_heads = 16\n",
    "  elif args.model_size == 'gpt2-large':\n",
    "    args.d = 1280\n",
    "    args.l = 36\n",
    "    args.num_heads = 20\n",
    "  else:\n",
    "    raise Exception(f'{args.model_size} is not supported.')\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7c0d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sonnet_path SONNET_PATH] [--held_out_sonnet_path HELD_OUT_SONNET_PATH]\n",
      "                             [--sonnet_out SONNET_OUT] [--seed SEED] [--epochs EPOCHS] [--use_gpu]\n",
      "                             [--temperature TEMPERATURE] [--top_p TOP_P] [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--model_size {gpt2,gpt2-medium,gpt2-large,gpt2-xl}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Home\\AppData\\Roaming\\jupyter\\runtime\\kernel-8e897a46-22fb-4440-90e1-787daeb95f74.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  args = get_args()\n",
    "  args.filepath = f'{args.epochs}-{args.lr}-sonnet.pt'  # 경로명 저장.\n",
    "  seed_everything(args.seed)  # 재현성을 위한 random seed 고정.\n",
    "  train(args)\n",
    "  generate_submission_sonnets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64325e25-9b00-493b-9789-714c9bca451d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (nlp_final)",
   "language": "python",
   "name": "nlp_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
