{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421bf5ec-a1ab-4ed5-9c9b-4bcf5102285c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#from modules.attention import CausalSelfAttention\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#같은 디렉토리안에 있는 데 modules.attention라고 하면 인식이 안되서 수정했다.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSelfAttention123\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CausalSelfAttention\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#입력 시퀀스를 받아서 Self-Attention과 FeedForward를 거치게 하는 클래스, init으로 초기화하고 foward함수를 사용하여 Self-Attention과 FeedForward를 적용\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSonnetLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[1;32m~\\JupyterNotebook\\실습\\nlp2025-1\\modules\\SelfAttention123.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoRALinear\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mLoRALinear\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'modules'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from LoRALinear import LoRALinear\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from modules.attention import CausalSelfAttention\n",
    "#같은 디렉토리안에 있는 데 modules.attention라고 하면 인식이 안되서 수정했다.\n",
    "from SelfAttention123 import SelfAttention123\n",
    "\n",
    "#입력 시퀀스를 받아서 Self-Attention과 FeedForward를 거치게 하는 클래스, init으로 초기화하고 foward함수를 사용하여 Self-Attention과 FeedForward를 적용\n",
    "class SonnetLayer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # Multi-head attention.\n",
    "      #SelfAttention123는 문맥을 이해하기 위한 self attention 함수인데 자신 이후의 토큰은 보지 못하도록 제한한다.\n",
    "    self.self_attention = SelfAttention123(config)\n",
    "      \n",
    "    # Add-norm for multi-head attention.\n",
    "      #다양한 문맥정보 추출을 위해 multi head attention을 하는 데 이 head마다의 attention 결과를 통합하는 게 attention_dense이다.\n",
    "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "      #self attention 결과를 정규화 수행하는 부분, nn.LayerNorm은 평균이 0, 분산이 1이 되도록 정규화해준다.\n",
    "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "      #과적합을 방지하기 위해서 훈련데이터 일부분을 무작위로 0으로 만들고 다른 것들을 높여서 맞춰주는 함수(훈련시에만 적용)\n",
    "      #hidden_dropout_prob가 0.1이면 10%를 무작위로 0으로 만든고 나머지는 1.11배\n",
    "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "      \n",
    "    # Feed forward.(ffn의 첫번째 선형변환)\n",
    "      \n",
    "      #입력 벡터(hidden_size 차원)를 더 넓은 차원(intermediate_size)으로 선형 확장하는 코드, 더 풍부한 비선형 표현을 학습하기 위해 잠깐 차원을 키우는 것\n",
    "      #나중에 가중치 W를 곱해주면서 원래 차원만큼 다시 작아질거임\n",
    "    self.interm_dense = LoRALinear(\n",
    "    config.hidden_size,\n",
    "    config.intermediate_size,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    "    )\n",
    "\n",
    "      #F.gelu는 GELU함수로 Relu와 유사하지만 더 부드럽게 통과시킨다. 입력이 작은 것은 더 약하게, 클수록 그대로 통과시키는 필터(더 유의미한 것을 통과시키는 필터)\n",
    "    self.interm_af = F.gelu\n",
    "    # Add-norm for feed forward.(ffn의 두번째 선형변환)\n",
    "      #차원을 늘리고 GELU를 활성화하고 다시 차원을 줄이는 데 이 코드가 차원을 줄이는 코드\n",
    "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "      \n",
    "      #ffn 끝에서 사용하는 layer Normalization(정규화)이다. 이전 정규화와 같이 평균을 0, 분산은 1로 맞추어준다.\n",
    "      #이전 attention의 정규화도 마찬가지로 이건 정규화 후 forward()에서 Residual을 적용할것임\n",
    "      #Residual은 입력값을 처리결과(Layer)에 더해서 전달하는 방식이다. ex) x = x + self.attention_dropout(self.attention_dense(attn_output))\n",
    "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "      #마찬가지로 과적합을 막기 위한 dropout\n",
    "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def add(self, input, output, dense_layer, dropout):\n",
    "    \"\"\"\n",
    "    TODO: forward() 함수를 위한 이 helper 메서드를 구현하시오:\n",
    "      - 이 함수는 multi-head attention layer와 feed forward layer 이후에 적용된다.\n",
    "      - GPT-2 layer는 각 sublayer의 변환된 출력에 드롭아웃을 적용한 후, 이를 sublayer 입력에 더한다. \n",
    "        이 함수에서는 Layer Normalization을 적용하지 않는다.\n",
    "    \"\"\"\n",
    "    out = dense_layer(output)\n",
    "    out = dropout(out)\n",
    "    return input + out  # Residual 연결 (정규화는 안 함)\n",
    "\n",
    "\n",
    "    #초기화에서 dropout까지 했으니 이후 Residual->정규화를 하면 된다.\n",
    "    #init에서의 초기화를 사용하는 데 예를 들어\n",
    "    #self.interm_dense = nn.Linear(768, 3072)\n",
    "    #self.interm_af = F.gelu\n",
    "    #self.out_dense = nn.Linear(3072, 768)\n",
    "    #라고 init에서 했으면\n",
    "    #x = self.interm_dense(x)     # 차원 확장 (768 → 3072)\n",
    "    #x = self.interm_af(x)        # 활성화 함수 (GELU)\n",
    "    #x = self.out_dense(x)        # 차원 축소 (3072 → 768)\n",
    "    #라고 foward에서 사용할거임\n",
    "    \n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    # --- 1. Self-Attention ---\n",
    "    normed_hidden = self.attention_layer_norm(hidden_states)\n",
    "    attn_output = self.self_attention(normed_hidden, attention_mask)\n",
    "    hidden_states = self.add(hidden_states, attn_output, self.attention_dense, self.attention_dropout)\n",
    "\n",
    "    # --- 2. Feed-Forward ---\n",
    "    normed_hidden = self.out_layer_norm(hidden_states)\n",
    "    ff_output = self.interm_dense(normed_hidden)\n",
    "    ff_output = self.interm_af(ff_output)\n",
    "    hidden_states = self.add(hidden_states, ff_output, self.out_dense, self.out_dropout)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "        \n",
    "  #def forward(self, hidden_states, attention_mask):\n",
    "    # \"\"\"\n",
    "    # TODO: forward pass의 구현. 고려해야 할 주요 사항은 다음과 같다:\n",
    "    #   - Multi-head Attention layer(SelfAttention123): mask된 입력을 기반으로 self-attention을 계산한다.\n",
    "    #   - Layer Normalization: Attention layer와 Feed-forward layer 이전에 적용된다.\n",
    "    #   - Dropout, Residual Connection, Layer Normalization를 적용하시오(self.add() 메서드를 사용)\n",
    "    #   - Feed-Forward layer: hidden states를 추가로 refine하기 위해 변환을 적용한다.\n",
    "    # \"\"\"\n",
    "\n",
    "    ### 완성시켜야 할 빈 코드 블록\n",
    "    #raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbbbd5-a6f4-4174-8344-333f46a22bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_final)",
   "language": "python",
   "name": "nlp_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
