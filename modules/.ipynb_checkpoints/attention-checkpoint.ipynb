{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_attention_heads = config.num_attention_heads\n",
        "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "    # key, value, query\uc5d0 \ub300\ud55c \uc120\ud615\ubcc0\ud658 layer \ucd08\uae30\ud654.\n",
        "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "    # \uc774 \ub4dc\ub86d\uc544\uc6c3\uc740 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \uc6d0\ub798 \uad6c\ud604\uc5d0 \ub530\ub77c normalized attention scores\uc5d0 \uc801\uc6a9\ub41c\ub2e4.\n",
        "    # \ub2e4\uc18c \uc774\ub840\uc801\uc774\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774\uac83\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \uc81c\uacf5\ud55c\ub2e4\uace0 \uc54c\ub824\uc838 \uc788\ub2e4.\n",
        "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "  def transform(self, x, linear_layer):\n",
        "    # hidden_state (x) \ub97c \uc0ac\uc601\ud558\uae30 \uc704\ud574 k, v, q\uc758 \ud574\ub2f9 linear_layer\uac00 \uc0ac\uc6a9\ub41c\ub2e4.\n",
        "    proj = linear_layer(x)\n",
        "    # \ub2e4\uc74c\uc73c\ub85c, \ud504\ub85c\uc81d\uc158\uc5d0 \ub300\ud574 \uc5ec\ub7ec \ud5e4\ub4dc\ub97c \uc0dd\uc131\ud574\uc57c \ud55c\ub2e4. \n",
        "    # \uc774\ub294 \uc740\ub2c9 \uc0c1\ud0dc\ub97c self.num_attention_heads\ub85c \ubd84\ud560\ud558\uba70, \n",
        "    # \uac01 \ud5e4\ub4dc\ub294 self.attention_head_size \ud06c\uae30\ub97c \uac16\ub3c4\ub85d \ud55c\ub2e4.\n",
        "    proj = rearrange(proj, 'b t (h d) -> b t h d', h=self.num_attention_heads)\n",
        "    # \uc801\uc808\ud788 \uc804\uce58\ud558\uc5ec \ud06c\uae30 [bs, num_attention_heads, seq_len, attention_head_size]\uc778 \ud504\ub85c\uc81d\uc158\uc744 \uc5bb\ub294\ub2e4.\n",
        "    proj = rearrange(proj, 'b t h d -> b h t d')\n",
        "    return proj\n",
        "\n",
        "  def attention(self, key, query, value, attention_mask):\n",
        "\n",
        "    ### \uc644\uc131\uc2dc\ucf1c\uc57c \ud560 \ube48 \ucf54\ub4dc \ube14\ub85d\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "  def forward(self, hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    hidden_states: [bs, seq_len, hidden_state]\n",
        "    attention_mask: [bs, 1, 1, seq_len]\n",
        "    output: [bs, seq_len, hidden_state]\n",
        "    \"\"\"\n",
        "    # \uba3c\uc800, self.transform\uc744 \uc0ac\uc6a9\ud558\uc5ec multi-head attention\uc5d0 \ud544\uc694\ud55c\n",
        "    # \uac01 \ud1a0\ud070\uc758 key, value, query\ub97c \uc0dd\uc131\ud574\uc57c \ud55c\ub2e4(\ud568\uc218 \ub0b4\ubd80\uc5d0 \uc790\uc138\ud55c \ub0b4\uc6a9 \uc788\uc74c).\n",
        "    # *_layer\uc758 \ud06c\uae30 = [bs, num_attention_heads, seq_len, attention_head_size].\n",
        "    key_layer = self.transform(hidden_states, self.key)\n",
        "    value_layer = self.transform(hidden_states, self.value)\n",
        "    query_layer = self.transform(hidden_states, self.query)\n",
        "    \n",
        "    # multi-head attention \uacc4\uc0b0.\n",
        "    attn_value = self.attention(key_layer, query_layer, value_layer, attention_mask)\n",
        "    return attn_value\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}