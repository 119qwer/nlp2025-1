from torch import nn

import torch.nn.functional as F

#from modules.attention import CausalSelfAttention
#ê°™ì€ ë””ë ‰í† ë¦¬ì•ˆì— ìˆëŠ” ë° modules.attentionë¼ê³  í•˜ë©´ ì¸ì‹ì´ ì•ˆë˜ì„œ ìˆ˜ì •í–ˆë‹¤.
from modules.attention import CausalSelfAttention

#ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ì„œ Self-Attentionê³¼ FeedForwardë¥¼ ê±°ì¹˜ê²Œ í•˜ëŠ” í´ë˜ìŠ¤, initìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³  fowardí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Self-Attentionê³¼ FeedForwardë¥¼ ì ìš©
class GPT2Layer(nn.Module):
  def __init__(self, config):
    super().__init__()
    # Multi-head attention.
      #CausalSelfAttentionëŠ” ë¬¸ë§¥ì„ ì´í•´í•˜ê¸° ìœ„í•œ self attention í•¨ìˆ˜ì¸ë° ìì‹  ì´í›„ì˜ í† í°ì€ ë³´ì§€ ëª»í•˜ë„ë¡ ì œí•œí•œë‹¤.
    self.self_attention = CausalSelfAttention(config)
      
    # Add-norm for multi-head attention.
      #ë‹¤ì–‘í•œ ë¬¸ë§¥ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ multi head attentionì„ í•˜ëŠ” ë° ì´ headë§ˆë‹¤ì˜ attention ê²°ê³¼ë¥¼ í†µí•©í•˜ëŠ” ê²Œ attention_denseì´ë‹¤.
    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)
      #self attention ê²°ê³¼ë¥¼ ì •ê·œí™” ìˆ˜í–‰í•˜ëŠ” ë¶€ë¶„, nn.LayerNormì€ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”í•´ì¤€ë‹¤.
    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
      #ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ í›ˆë ¨ë°ì´í„° ì¼ë¶€ë¶„ì„ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“¤ê³  ë‹¤ë¥¸ ê²ƒë“¤ì„ ë†’ì—¬ì„œ ë§ì¶°ì£¼ëŠ” í•¨ìˆ˜(í›ˆë ¨ì‹œì—ë§Œ ì ìš©)
      #hidden_dropout_probê°€ 0.1ì´ë©´ 10%ë¥¼ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“ ê³  ë‚˜ë¨¸ì§€ëŠ” 1.11ë°°
    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)
      
    # Feed forward.(ffnì˜ ì²«ë²ˆì§¸ ì„ í˜•ë³€í™˜)
      
      #ì…ë ¥ ë²¡í„°(hidden_size ì°¨ì›)ë¥¼ ë” ë„“ì€ ì°¨ì›(intermediate_size)ìœ¼ë¡œ ì„ í˜• í™•ì¥í•˜ëŠ” ì½”ë“œ, ë” í’ë¶€í•œ ë¹„ì„ í˜• í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì ê¹ ì°¨ì›ì„ í‚¤ìš°ëŠ” ê²ƒ
      #ë‚˜ì¤‘ì— ê°€ì¤‘ì¹˜ Wë¥¼ ê³±í•´ì£¼ë©´ì„œ ì›ë˜ ì°¨ì›ë§Œí¼ ë‹¤ì‹œ ì‘ì•„ì§ˆê±°ì„
    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)
      #F.geluëŠ” GELUí•¨ìˆ˜ë¡œ Reluì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ë¶€ë“œëŸ½ê²Œ í†µê³¼ì‹œí‚¨ë‹¤. ì…ë ¥ì´ ì‘ì€ ê²ƒì€ ë” ì•½í•˜ê²Œ, í´ìˆ˜ë¡ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¤ëŠ” í•„í„°(ë” ìœ ì˜ë¯¸í•œ ê²ƒì„ í†µê³¼ì‹œí‚¤ëŠ” í•„í„°)
    self.interm_af = F.gelu
    # Add-norm for feed forward.(ffnì˜ ë‘ë²ˆì§¸ ì„ í˜•ë³€í™˜)
      #ì°¨ì›ì„ ëŠ˜ë¦¬ê³  GELUë¥¼ í™œì„±í™”í•˜ê³  ë‹¤ì‹œ ì°¨ì›ì„ ì¤„ì´ëŠ” ë° ì´ ì½”ë“œê°€ ì°¨ì›ì„ ì¤„ì´ëŠ” ì½”ë“œ
    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)
      
      #ffn ëì—ì„œ ì‚¬ìš©í•˜ëŠ” layer Normalization(ì •ê·œí™”)ì´ë‹¤. ì´ì „ ì •ê·œí™”ì™€ ê°™ì´ í‰ê· ì„ 0, ë¶„ì‚°ì€ 1ë¡œ ë§ì¶”ì–´ì¤€ë‹¤.
      #ì´ì „ attentionì˜ ì •ê·œí™”ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì´ê±´ ì •ê·œí™” í›„ forward()ì—ì„œ Residualì„ ì ìš©í• ê²ƒì„
      #Residualì€ ì…ë ¥ê°’ì„ ì²˜ë¦¬ê²°ê³¼(Layer)ì— ë”í•´ì„œ ì „ë‹¬í•˜ëŠ” ë°©ì‹ì´ë‹¤. ex) x = x + self.attention_dropout(self.attention_dense(attn_output))
    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
      #ë§ˆì°¬ê°€ì§€ë¡œ ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•œ dropout
    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)

  def add(self, input, output, dense_layer, dropout):
    """
    TODO: forward() í•¨ìˆ˜ë¥¼ ìœ„í•œ ì´ helper ë©”ì„œë“œë¥¼ êµ¬í˜„í•˜ì‹œì˜¤:
      - ì´ í•¨ìˆ˜ëŠ” multi-head attention layerì™€ feed forward layer ì´í›„ì— ì ìš©ëœë‹¤.
      - GPT-2 layerëŠ” ê° sublayerì˜ ë³€í™˜ëœ ì¶œë ¥ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•œ í›„, ì´ë¥¼ sublayer ì…ë ¥ì— ë”í•œë‹¤. 
        ì´ í•¨ìˆ˜ì—ì„œëŠ” Layer Normalizationì„ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    out = dense_layer(output)
    out = dropout(out)
    return input + out  # Residual ì—°ê²° (ì •ê·œí™”ëŠ” ì•ˆ í•¨)


    #ì´ˆê¸°í™”ì—ì„œ dropoutê¹Œì§€ í–ˆìœ¼ë‹ˆ ì´í›„ Residual->ì •ê·œí™”ë¥¼ í•˜ë©´ ëœë‹¤.
    #initì—ì„œì˜ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ì˜ˆë¥¼ ë“¤ì–´
    #self.interm_dense = nn.Linear(768, 3072)
    #self.interm_af = F.gelu
    #self.out_dense = nn.Linear(3072, 768)
    #ë¼ê³  initì—ì„œ í–ˆìœ¼ë©´
    #x = self.interm_dense(x)     # ì°¨ì› í™•ì¥ (768 â†’ 3072)
    #x = self.interm_af(x)        # í™œì„±í™” í•¨ìˆ˜ (GELU)
    #x = self.out_dense(x)        # ì°¨ì› ì¶•ì†Œ (3072 â†’ 768)
    #ë¼ê³  fowardì—ì„œ ì‚¬ìš©í• ê±°ì„
    
  def forward(self, hidden_states, attention_mask):
    # -------------------------------
    # 1. Self-Attention Block
    # -------------------------------
    residual = hidden_states  # ğŸ”¹ ì”ì°¨ìš© ë³µì‚¬

    # ğŸ”¸ Self-Attention ì—°ì‚° (ì¦‰, self_attentionì˜ forward í•¨ìˆ˜ ì‹¤í–‰)
    attention_output = self.self_attention(
        hidden_states,        # Q, K, V
        attention_mask        # maskëŠ” í† í°ê°„ ì£¼ì˜ë¥¼ ì œí•œíˆê¸° ìœ„í•¨(ì›í•˜ëŠ” í† í°ë§Œ ë³´ê¸° ìœ„í•¨)
    )

    # ğŸ”¸ í›„ì²˜ë¦¬: íˆ¬ì˜(headë“¤ì˜ attention í†µí•©) â†’ ë“œë¡­ì•„ì›ƒ(ê³¼ì í•© ë°©ì§€)
    attention_output = self.add(residual, attention_output, self.attention_dense, self.attention_dropout)
    #attention_output = self.attention_dense(attention_output)
    #attention_output = self.attention_dropout(attention_output)

    # ğŸ”¸ Residual + LayerNorm (residual í›„ì— ì •ê·œí™” í•˜ëŠ” ê²ƒì´ ì›ì¹™, ì•„ë‹ˆë©´ residualì˜ íš¨ê³¼ê°€ ë°˜ê°ë¨)
    hidden_states = self.attention_layer_norm(attention_output)

    # -------------------------------
    # 2. Feed Forward Network Block(Self-Attentionì„ í†µí•´ í† í° ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•œ í›„ ffnìœ¼ë¡œ ë¹„ì„ í˜•ì ìœ¼ë¡œ ê°€ê³µ)
    # -------------------------------
    residual = hidden_states  # ğŸ”¹ ë‹¤ì‹œ ì”ì°¨ìš© ë³µì‚¬

    # ğŸ”¸ FFN í™•ì¥ â†’ í™œì„±í™” â†’ ì¶•ì†Œ
    ff_output = self.interm_dense(hidden_states) # ì°¨ì› í™•ì¥
    ff_output = self.interm_af(ff_output)       # GELUí•¨ìˆ˜ ì ìš©
    ff_output = self.add(residual, ff_output, self.out_dense, self.out_dropout)
    #ff_output = self.out_dense(ff_output)       #ì°¨ì› ì¶•ì†Œ
    #ff_output = self.out_dropout(ff_output)     #ë“œëì•„ì›ƒ

    # ğŸ”¸ Residual + LayerNorm
    hidden_states = self.out_layer_norm(ff_output)

    # ğŸ”š ìµœì¢… ì¶œë ¥
    return hidden_states
        
  #def forward(self, hidden_states, attention_mask):
    # """
    # TODO: forward passì˜ êµ¬í˜„. ê³ ë ¤í•´ì•¼ í•  ì£¼ìš” ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:
    #   - Multi-head Attention layer(CausalSelfAttention): maskëœ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ self-attentionì„ ê³„ì‚°í•œë‹¤.
    #   - Layer Normalization: Attention layerì™€ Feed-forward layer ì´ì „ì— ì ìš©ëœë‹¤.
    #   - Dropout, Residual Connection, Layer Normalizationë¥¼ ì ìš©í•˜ì‹œì˜¤(self.add() ë©”ì„œë“œë¥¼ ì‚¬ìš©)
    #   - Feed-Forward layer: hidden statesë¥¼ ì¶”ê°€ë¡œ refineí•˜ê¸° ìœ„í•´ ë³€í™˜ì„ ì ìš©í•œë‹¤.
    # """

    ### ì™„ì„±ì‹œì¼œì•¼ í•  ë¹ˆ ì½”ë“œ ë¸”ë¡
    #raise NotImplementedError

import torch
import torch.nn as nn
import torch.nn.functional as F

# í…ŒìŠ¤íŠ¸ìš© config í´ë˜ìŠ¤ ì •ì˜
class Config:
    def __init__(self):
        self.hidden_size = 768
        self.intermediate_size = 3072
        self.hidden_dropout_prob = 0.1
        self.attention_probs_dropout_prob = 0.1
        self.layer_norm_eps = 1e-5
        self.max_position_embeddings = 128
        self.n_head = 12
        # â—ï¸ì¶”ê°€í•´ ì£¼ì„¸ìš”:
        self.num_attention_heads = 12  # ë˜ëŠ” self.n_headë„ ê°™ì´ ë‘˜ ìˆ˜ ìˆìŒ
        self.max_position_embeddings = 128

# ê°€ì§œ causal mask ìƒì„± í•¨ìˆ˜
def generate_causal_mask(seq_len):
    return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, T, T]

# í…ŒìŠ¤íŠ¸ìš© GPT2Layer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
config = Config()
layer = GPT2Layer(config)

# ì…ë ¥ í…ì„œ ìƒì„±: [batch, seq_len, hidden_size]
batch_size = 2
seq_len = 16
hidden_size = config.hidden_size
x = torch.randn(batch_size, seq_len, hidden_size)

# ë§ˆìŠ¤í¬ ìƒì„±
attention_mask = generate_causal_mask(seq_len)

# forward ì‹¤í–‰
with torch.no_grad():
    out = layer(x, attention_mask)

print("ì…ë ¥ shape:", x.shape)
print("ì¶œë ¥ shape:", out.shape)

