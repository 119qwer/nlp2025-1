{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from modules.attention import CausalSelfAttention\n",
    "#ê°™ì€ ë””ë ‰í† ë¦¬ì•ˆì— ìˆëŠ” ë° modules.attentionë¼ê³  í•˜ë©´ ì¸ì‹ì´ ì•ˆë˜ì„œ ìˆ˜ì •í–ˆë‹¤.\n",
    "from attention import CausalSelfAttention\n",
    "\n",
    "#ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ì„œ Self-Attentionê³¼ FeedForwardë¥¼ ê±°ì¹˜ê²Œ í•˜ëŠ” í´ë˜ìŠ¤, initìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³  fowardí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Self-Attentionê³¼ FeedForwardë¥¼ ì ìš©\n",
    "class GPT2Layer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # Multi-head attention.\n",
    "      #CausalSelfAttentionëŠ” ë¬¸ë§¥ì„ ì´í•´í•˜ê¸° ìœ„í•œ self attention í•¨ìˆ˜ì¸ë° ìì‹  ì´í›„ì˜ í† í°ì€ ë³´ì§€ ëª»í•˜ë„ë¡ ì œí•œí•œë‹¤.\n",
    "    self.self_attention = CausalSelfAttention(config)\n",
    "      \n",
    "    # Add-norm for multi-head attention.\n",
    "      #ë‹¤ì–‘í•œ ë¬¸ë§¥ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ multi head attentionì„ í•˜ëŠ” ë° ì´ headë§ˆë‹¤ì˜ attention ê²°ê³¼ë¥¼ í†µí•©í•˜ëŠ” ê²Œ attention_denseì´ë‹¤.\n",
    "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "      #self attention ê²°ê³¼ë¥¼ ì •ê·œí™” ìˆ˜í–‰í•˜ëŠ” ë¶€ë¶„, nn.LayerNormì€ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”í•´ì¤€ë‹¤.\n",
    "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "      #ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ í›ˆë ¨ë°ì´í„° ì¼ë¶€ë¶„ì„ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“¤ê³  ë‹¤ë¥¸ ê²ƒë“¤ì„ ë†’ì—¬ì„œ ë§ì¶°ì£¼ëŠ” í•¨ìˆ˜(í›ˆë ¨ì‹œì—ë§Œ ì ìš©)\n",
    "      #hidden_dropout_probê°€ 0.1ì´ë©´ 10%ë¥¼ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“ ê³  ë‚˜ë¨¸ì§€ëŠ” 1.11ë°°\n",
    "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "      \n",
    "    # Feed forward.(ffnì˜ ì²«ë²ˆì§¸ ì„ í˜•ë³€í™˜)\n",
    "      \n",
    "      #ì…ë ¥ ë²¡í„°(hidden_size ì°¨ì›)ë¥¼ ë” ë„“ì€ ì°¨ì›(intermediate_size)ìœ¼ë¡œ ì„ í˜• í™•ì¥í•˜ëŠ” ì½”ë“œ, ë” í’ë¶€í•œ ë¹„ì„ í˜• í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì ê¹ ì°¨ì›ì„ í‚¤ìš°ëŠ” ê²ƒ\n",
    "      #ë‚˜ì¤‘ì— ê°€ì¤‘ì¹˜ Wë¥¼ ê³±í•´ì£¼ë©´ì„œ ì›ë˜ ì°¨ì›ë§Œí¼ ë‹¤ì‹œ ì‘ì•„ì§ˆê±°ì„\n",
    "    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "      #F.geluëŠ” GELUí•¨ìˆ˜ë¡œ Reluì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ë¶€ë“œëŸ½ê²Œ í†µê³¼ì‹œí‚¨ë‹¤. ì…ë ¥ì´ ì‘ì€ ê²ƒì€ ë” ì•½í•˜ê²Œ, í´ìˆ˜ë¡ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¤ëŠ” í•„í„°(ë” ìœ ì˜ë¯¸í•œ ê²ƒì„ í†µê³¼ì‹œí‚¤ëŠ” í•„í„°)\n",
    "    self.interm_af = F.gelu\n",
    "    # Add-norm for feed forward.(ffnì˜ ë‘ë²ˆì§¸ ì„ í˜•ë³€í™˜)\n",
    "      #ì°¨ì›ì„ ëŠ˜ë¦¬ê³  GELUë¥¼ í™œì„±í™”í•˜ê³  ë‹¤ì‹œ ì°¨ì›ì„ ì¤„ì´ëŠ” ë° ì´ ì½”ë“œê°€ ì°¨ì›ì„ ì¤„ì´ëŠ” ì½”ë“œ\n",
    "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "      \n",
    "      #ffn ëì—ì„œ ì‚¬ìš©í•˜ëŠ” layer Normalization(ì •ê·œí™”)ì´ë‹¤. ì´ì „ ì •ê·œí™”ì™€ ê°™ì´ í‰ê· ì„ 0, ë¶„ì‚°ì€ 1ë¡œ ë§ì¶”ì–´ì¤€ë‹¤.\n",
    "      #ì´ì „ attentionì˜ ì •ê·œí™”ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì´ê±´ ì •ê·œí™” í›„ forward()ì—ì„œ Residualì„ ì ìš©í• ê²ƒì„\n",
    "      #Residualì€ ì…ë ¥ê°’ì„ ì²˜ë¦¬ê²°ê³¼(Layer)ì— ë”í•´ì„œ ì „ë‹¬í•˜ëŠ” ë°©ì‹ì´ë‹¤. ex) x = x + self.attention_dropout(self.attention_dense(attn_output))\n",
    "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "      #ë§ˆì°¬ê°€ì§€ë¡œ ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•œ dropout\n",
    "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def add(self, input, output, dense_layer, dropout):\n",
    "    \"\"\"\n",
    "    TODO: forward() í•¨ìˆ˜ë¥¼ ìœ„í•œ ì´ helper ë©”ì„œë“œë¥¼ êµ¬í˜„í•˜ì‹œì˜¤:\n",
    "      - ì´ í•¨ìˆ˜ëŠ” multi-head attention layerì™€ feed forward layer ì´í›„ì— ì ìš©ëœë‹¤.\n",
    "      - GPT-2 layerëŠ” ê° sublayerì˜ ë³€í™˜ëœ ì¶œë ¥ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•œ í›„, ì´ë¥¼ sublayer ì…ë ¥ì— ë”í•œë‹¤. \n",
    "        ì´ í•¨ìˆ˜ì—ì„œëŠ” Layer Normalizationì„ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "    \"\"\"\n",
    "    out = dense_layer(output)\n",
    "    out = dropout(out)\n",
    "    return input + out  # Residual ì—°ê²° (ì •ê·œí™”ëŠ” ì•ˆ í•¨)\n",
    "\n",
    "\n",
    "    #ì´ˆê¸°í™”ì—ì„œ dropoutê¹Œì§€ í–ˆìœ¼ë‹ˆ ì´í›„ Residual->ì •ê·œí™”ë¥¼ í•˜ë©´ ëœë‹¤.\n",
    "    #initì—ì„œì˜ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ì˜ˆë¥¼ ë“¤ì–´\n",
    "    #self.interm_dense = nn.Linear(768, 3072)\n",
    "    #self.interm_af = F.gelu\n",
    "    #self.out_dense = nn.Linear(3072, 768)\n",
    "    #ë¼ê³  initì—ì„œ í–ˆìœ¼ë©´\n",
    "    #x = self.interm_dense(x)     # ì°¨ì› í™•ì¥ (768 â†’ 3072)\n",
    "    #x = self.interm_af(x)        # í™œì„±í™” í•¨ìˆ˜ (GELU)\n",
    "    #x = self.out_dense(x)        # ì°¨ì› ì¶•ì†Œ (3072 â†’ 768)\n",
    "    #ë¼ê³  fowardì—ì„œ ì‚¬ìš©í• ê±°ì„\n",
    "    \n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    # -------------------------------\n",
    "    # 1. Self-Attention Block\n",
    "    # -------------------------------\n",
    "    residual = hidden_states  # ğŸ”¹ ì”ì°¨ìš© ë³µì‚¬\n",
    "\n",
    "    # ğŸ”¸ Self-Attention ì—°ì‚° (ì¦‰, self_attentionì˜ forward í•¨ìˆ˜ ì‹¤í–‰)\n",
    "    attention_output = self.self_attention(\n",
    "        hidden_states,        # Q, K, V\n",
    "        attention_mask        # maskëŠ” í† í°ê°„ ì£¼ì˜ë¥¼ ì œí•œíˆê¸° ìœ„í•¨(ì›í•˜ëŠ” í† í°ë§Œ ë³´ê¸° ìœ„í•¨)\n",
    "    )\n",
    "\n",
    "    # ğŸ”¸ í›„ì²˜ë¦¬: íˆ¬ì˜(headë“¤ì˜ attention í†µí•©) â†’ ë“œë¡­ì•„ì›ƒ(ê³¼ì í•© ë°©ì§€)\n",
    "    attention_output = self.add(residual, attention_output, self.attention_dense, self.attention_dropout)\n",
    "    #attention_output = self.attention_dense(attention_output)\n",
    "    #attention_output = self.attention_dropout(attention_output)\n",
    "\n",
    "    # ğŸ”¸ Residual + LayerNorm (residual í›„ì— ì •ê·œí™” í•˜ëŠ” ê²ƒì´ ì›ì¹™, ì•„ë‹ˆë©´ residualì˜ íš¨ê³¼ê°€ ë°˜ê°ë¨)\n",
    "    hidden_states = self.attention_layer_norm(attention_output)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Feed Forward Network Block(Self-Attentionì„ í†µí•´ í† í° ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•œ í›„ ffnìœ¼ë¡œ ë¹„ì„ í˜•ì ìœ¼ë¡œ ê°€ê³µ)\n",
    "    # -------------------------------\n",
    "    residual = hidden_states  # ğŸ”¹ ë‹¤ì‹œ ì”ì°¨ìš© ë³µì‚¬\n",
    "\n",
    "    # ğŸ”¸ FFN í™•ì¥ â†’ í™œì„±í™” â†’ ì¶•ì†Œ\n",
    "    ff_output = self.interm_dense(hidden_states) # ì°¨ì› í™•ì¥\n",
    "    ff_output = self.interm_af(ff_output)       # GELUí•¨ìˆ˜ ì ìš©\n",
    "    ff_output = self.add(residual, ff_output, self.out_dense, self.out_dropout)\n",
    "    #ff_output = self.out_dense(ff_output)       #ì°¨ì› ì¶•ì†Œ\n",
    "    #ff_output = self.out_dropout(ff_output)     #ë“œëì•„ì›ƒ\n",
    "\n",
    "    # ğŸ”¸ Residual + LayerNorm\n",
    "    hidden_states = self.out_layer_norm(ff_output)\n",
    "\n",
    "    # ğŸ”š ìµœì¢… ì¶œë ¥\n",
    "    return hidden_states\n",
    "        \n",
    "  #def forward(self, hidden_states, attention_mask):\n",
    "    # \"\"\"\n",
    "    # TODO: forward passì˜ êµ¬í˜„. ê³ ë ¤í•´ì•¼ í•  ì£¼ìš” ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:\n",
    "    #   - Multi-head Attention layer(CausalSelfAttention): maskëœ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ self-attentionì„ ê³„ì‚°í•œë‹¤.\n",
    "    #   - Layer Normalization: Attention layerì™€ Feed-forward layer ì´ì „ì— ì ìš©ëœë‹¤.\n",
    "    #   - Dropout, Residual Connection, Layer Normalizationë¥¼ ì ìš©í•˜ì‹œì˜¤(self.add() ë©”ì„œë“œë¥¼ ì‚¬ìš©)\n",
    "    #   - Feed-Forward layer: hidden statesë¥¼ ì¶”ê°€ë¡œ refineí•˜ê¸° ìœ„í•´ ë³€í™˜ì„ ì ìš©í•œë‹¤.\n",
    "    # \"\"\"\n",
    "\n",
    "    ### ì™„ì„±ì‹œì¼œì•¼ í•  ë¹ˆ ì½”ë“œ ë¸”ë¡\n",
    "    #raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# forward ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì…ë ¥ shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì¶œë ¥ shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m, in \u001b[0;36mGPT2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m     70\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states  \u001b[38;5;66;03m# ğŸ”¹ ì”ì°¨ìš© ë³µì‚¬\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# ğŸ”¸ Self-Attention ì—°ì‚°\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Q, K, V\u001b[39;49;00m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# maskëŠ” í† í°ê°„ ì£¼ì˜ë¥¼ ì œí•œíˆê¸° ìœ„í•¨(ì›í•˜ëŠ” í† í°ë§Œ ë³´ê¸° ìœ„í•¨)\u001b[39;49;00m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# ğŸ”¸ í›„ì²˜ë¦¬: íˆ¬ì˜(headë“¤ì˜ attention í†µí•©) â†’ ë“œë¡­ì•„ì›ƒ(ê³¼ì í•© ë°©ì§€)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(residual, attention_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dense, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\JupyterNotebook\\ì‹¤ìŠµ\\nlp2025-1\\modules\\attention.py:55\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m     52\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# multi-head attention ê³„ì‚°.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m attn_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_value\n",
      "File \u001b[1;32m~\\JupyterNotebook\\ì‹¤ìŠµ\\nlp2025-1\\modules\\attention.py:38\u001b[0m, in \u001b[0;36mCausalSelfAttention.attention\u001b[1;34m(self, key, query, value, attention_mask)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, query, value, attention_mask):\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m   \u001b[38;5;66;03m### ì™„ì„±ì‹œì¼œì•¼ í•  ë¹ˆ ì½”ë“œ ë¸”ë¡\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© config í´ë˜ìŠ¤ ì •ì˜\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 768\n",
    "        self.intermediate_size = 3072\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.max_position_embeddings = 128\n",
    "        self.n_head = 12\n",
    "        # â—ï¸ì¶”ê°€í•´ ì£¼ì„¸ìš”:\n",
    "        self.num_attention_heads = 12  # ë˜ëŠ” self.n_headë„ ê°™ì´ ë‘˜ ìˆ˜ ìˆìŒ\n",
    "        self.max_position_embeddings = 128\n",
    "\n",
    "# ê°€ì§œ causal mask ìƒì„± í•¨ìˆ˜\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, T, T]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© GPT2Layer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "config = Config()\n",
    "layer = GPT2Layer(config)\n",
    "\n",
    "# ì…ë ¥ í…ì„œ ìƒì„±: [batch, seq_len, hidden_size]\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "hidden_size = config.hidden_size\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# ë§ˆìŠ¤í¬ ìƒì„±\n",
    "attention_mask = generate_causal_mask(seq_len)\n",
    "\n",
    "# forward ì‹¤í–‰\n",
    "with torch.no_grad():\n",
    "    out = layer(x, attention_mask)\n",
    "\n",
    "print(\"ì…ë ¥ shape:\", x.shape)\n",
    "print(\"ì¶œë ¥ shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (nlp_final)",
   "language": "python",
   "name": "nlp_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
