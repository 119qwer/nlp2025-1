{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from modules.attention import CausalSelfAttention\n",
    "#같은 디렉토리안에 있는 데 modules.attention라고 하면 인식이 안되서 수정했다.\n",
    "from attention import CausalSelfAttention\n",
    "\n",
    "#입력 시퀀스를 받아서 Self-Attention과 FeedForward를 거치게 하는 클래스, init으로 초기화하고 foward함수를 사용하여 Self-Attention과 FeedForward를 적용\n",
    "class GPT2Layer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # Multi-head attention.\n",
    "      #CausalSelfAttention는 문맥을 이해하기 위한 self attention 함수인데 자신 이후의 토큰은 보지 못하도록 제한한다.\n",
    "    self.self_attention = CausalSelfAttention(config)\n",
    "      \n",
    "    # Add-norm for multi-head attention.\n",
    "      #다양한 문맥정보 추출을 위해 multi head attention을 하는 데 이 head마다의 attention 결과를 통합하는 게 attention_dense이다.\n",
    "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "      #self attention 결과를 정규화 수행하는 부분, nn.LayerNorm은 평균이 0, 분산이 1이 되도록 정규화해준다.\n",
    "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "      #과적합을 방지하기 위해서 훈련데이터 일부분을 무작위로 0으로 만들고 다른 것들을 높여서 맞춰주는 함수(훈련시에만 적용)\n",
    "      #hidden_dropout_prob가 0.1이면 10%를 무작위로 0으로 만든고 나머지는 1.11배\n",
    "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "      \n",
    "    # Feed forward.(ffn의 첫번째 선형변환)\n",
    "      \n",
    "      #입력 벡터(hidden_size 차원)를 더 넓은 차원(intermediate_size)으로 선형 확장하는 코드, 더 풍부한 비선형 표현을 학습하기 위해 잠깐 차원을 키우는 것\n",
    "      #나중에 가중치 W를 곱해주면서 원래 차원만큼 다시 작아질거임\n",
    "    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "      #F.gelu는 GELU함수로 Relu와 유사하지만 더 부드럽게 통과시킨다. 입력이 작은 것은 더 약하게, 클수록 그대로 통과시키는 필터(더 유의미한 것을 통과시키는 필터)\n",
    "    self.interm_af = F.gelu\n",
    "    # Add-norm for feed forward.(ffn의 두번째 선형변환)\n",
    "      #차원을 늘리고 GELU를 활성화하고 다시 차원을 줄이는 데 이 코드가 차원을 줄이는 코드\n",
    "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "      \n",
    "      #ffn 끝에서 사용하는 layer Normalization(정규화)이다. 이전 정규화와 같이 평균을 0, 분산은 1로 맞추어준다.\n",
    "      #이전 attention의 정규화도 마찬가지로 이건 정규화 후 forward()에서 Residual을 적용할것임\n",
    "      #Residual은 입력값을 처리결과(Layer)에 더해서 전달하는 방식이다. ex) x = x + self.attention_dropout(self.attention_dense(attn_output))\n",
    "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "      #마찬가지로 과적합을 막기 위한 dropout\n",
    "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def add(self, input, output, dense_layer, dropout):\n",
    "    \"\"\"\n",
    "    TODO: forward() 함수를 위한 이 helper 메서드를 구현하시오:\n",
    "      - 이 함수는 multi-head attention layer와 feed forward layer 이후에 적용된다.\n",
    "      - GPT-2 layer는 각 sublayer의 변환된 출력에 드롭아웃을 적용한 후, 이를 sublayer 입력에 더한다. \n",
    "        이 함수에서는 Layer Normalization을 적용하지 않는다.\n",
    "    \"\"\"\n",
    "    out = dense_layer(output)\n",
    "    out = dropout(out)\n",
    "    return input + out  # Residual 연결 (정규화는 안 함)\n",
    "\n",
    "\n",
    "    #초기화에서 dropout까지 했으니 이후 Residual->정규화를 하면 된다.\n",
    "    #init에서의 초기화를 사용하는 데 예를 들어\n",
    "    #self.interm_dense = nn.Linear(768, 3072)\n",
    "    #self.interm_af = F.gelu\n",
    "    #self.out_dense = nn.Linear(3072, 768)\n",
    "    #라고 init에서 했으면\n",
    "    #x = self.interm_dense(x)     # 차원 확장 (768 → 3072)\n",
    "    #x = self.interm_af(x)        # 활성화 함수 (GELU)\n",
    "    #x = self.out_dense(x)        # 차원 축소 (3072 → 768)\n",
    "    #라고 foward에서 사용할거임\n",
    "    \n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    # -------------------------------\n",
    "    # 1. Self-Attention Block\n",
    "    # -------------------------------\n",
    "    residual = hidden_states  # 🔹 잔차용 복사\n",
    "\n",
    "    # 🔸 Self-Attention 연산 (즉, self_attention의 forward 함수 실행)\n",
    "    attention_output = self.self_attention(\n",
    "        hidden_states,        # Q, K, V\n",
    "        attention_mask        # mask는 토큰간 주의를 제한히기 위함(원하는 토큰만 보기 위함)\n",
    "    )\n",
    "\n",
    "    # 🔸 후처리: 투영(head들의 attention 통합) → 드롭아웃(과적합 방지)\n",
    "    attention_output = self.add(residual, attention_output, self.attention_dense, self.attention_dropout)\n",
    "    #attention_output = self.attention_dense(attention_output)\n",
    "    #attention_output = self.attention_dropout(attention_output)\n",
    "\n",
    "    # 🔸 Residual + LayerNorm (residual 후에 정규화 하는 것이 원칙, 아니면 residual의 효과가 반감됨)\n",
    "    hidden_states = self.attention_layer_norm(attention_output)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Feed Forward Network Block(Self-Attention을 통해 토큰 간 관계를 학습한 후 ffn으로 비선형적으로 가공)\n",
    "    # -------------------------------\n",
    "    residual = hidden_states  # 🔹 다시 잔차용 복사\n",
    "\n",
    "    # 🔸 FFN 확장 → 활성화 → 축소\n",
    "    ff_output = self.interm_dense(hidden_states) # 차원 확장\n",
    "    ff_output = self.interm_af(ff_output)       # GELU함수 적용\n",
    "    ff_output = self.add(residual, ff_output, self.out_dense, self.out_dropout)\n",
    "    #ff_output = self.out_dense(ff_output)       #차원 축소\n",
    "    #ff_output = self.out_dropout(ff_output)     #드랍아웃\n",
    "\n",
    "    # 🔸 Residual + LayerNorm\n",
    "    hidden_states = self.out_layer_norm(ff_output)\n",
    "\n",
    "    # 🔚 최종 출력\n",
    "    return hidden_states\n",
    "        \n",
    "  #def forward(self, hidden_states, attention_mask):\n",
    "    # \"\"\"\n",
    "    # TODO: forward pass의 구현. 고려해야 할 주요 사항은 다음과 같다:\n",
    "    #   - Multi-head Attention layer(CausalSelfAttention): mask된 입력을 기반으로 self-attention을 계산한다.\n",
    "    #   - Layer Normalization: Attention layer와 Feed-forward layer 이전에 적용된다.\n",
    "    #   - Dropout, Residual Connection, Layer Normalization를 적용하시오(self.add() 메서드를 사용)\n",
    "    #   - Feed-Forward layer: hidden states를 추가로 refine하기 위해 변환을 적용한다.\n",
    "    # \"\"\"\n",
    "\n",
    "    ### 완성시켜야 할 빈 코드 블록\n",
    "    #raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# forward 실행\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m입력 shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m출력 shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m, in \u001b[0;36mGPT2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m     70\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states  \u001b[38;5;66;03m# 🔹 잔차용 복사\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 🔸 Self-Attention 연산\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Q, K, V\u001b[39;49;00m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# mask는 토큰간 주의를 제한히기 위함(원하는 토큰만 보기 위함)\u001b[39;49;00m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# 🔸 후처리: 투영(head들의 attention 통합) → 드롭아웃(과적합 방지)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(residual, attention_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dense, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\nlp_final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\JupyterNotebook\\실습\\nlp2025-1\\modules\\attention.py:55\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m     52\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# multi-head attention 계산.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m attn_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_value\n",
      "File \u001b[1;32m~\\JupyterNotebook\\실습\\nlp2025-1\\modules\\attention.py:38\u001b[0m, in \u001b[0;36mCausalSelfAttention.attention\u001b[1;34m(self, key, query, value, attention_mask)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, query, value, attention_mask):\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m   \u001b[38;5;66;03m### 완성시켜야 할 빈 코드 블록\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 테스트용 config 클래스 정의\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 768\n",
    "        self.intermediate_size = 3072\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.max_position_embeddings = 128\n",
    "        self.n_head = 12\n",
    "        # ❗️추가해 주세요:\n",
    "        self.num_attention_heads = 12  # 또는 self.n_head도 같이 둘 수 있음\n",
    "        self.max_position_embeddings = 128\n",
    "\n",
    "# 가짜 causal mask 생성 함수\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, T, T]\n",
    "\n",
    "# 테스트용 GPT2Layer 인스턴스 생성\n",
    "config = Config()\n",
    "layer = GPT2Layer(config)\n",
    "\n",
    "# 입력 텐서 생성: [batch, seq_len, hidden_size]\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "hidden_size = config.hidden_size\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# 마스크 생성\n",
    "attention_mask = generate_causal_mask(seq_len)\n",
    "\n",
    "# forward 실행\n",
    "with torch.no_grad():\n",
    "    out = layer(x, attention_mask)\n",
    "\n",
    "print(\"입력 shape:\", x.shape)\n",
    "print(\"출력 shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (nlp_final)",
   "language": "python",
   "name": "nlp_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
